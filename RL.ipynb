{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cceeb87-4ea7-452e-9517-972271909dfa",
   "metadata": {},
   "source": [
    "# RL\n",
    "\n",
    "The goal of this notebook is to introduce an RL agent into EdgeSimPy. \n",
    "\n",
    "How:\n",
    "- Pass the rl agent to the simulator through the instantiation: \n",
    "```python\n",
    "simulator = esp.Simulator(tick_duration=1,tick_unit=\"seconds\",\n",
    "    stopping_criterion=stop_on_n_steps,\n",
    "    resource_management_algorithm=RL,\n",
    "    scheduler=esp.activation_schedulers.ChaosScheduler,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053c00c-5957-490b-9406-972c3875edca",
   "metadata": {},
   "source": [
    "Since I'll be building this with the Ray library, I'll need a new environment. \n",
    "\n",
    "I have one named `Python (edgesimpy)`\n",
    "\n",
    "This environment will not have edgesimpy in it's pip list or conda list. \n",
    "\n",
    "This environment will be for edge chaos. \n",
    "\n",
    "Just to say it, I named the environment I used before edgesympy_tut, but passed the name `Python (edgesimpy)` to the jupyter kernel. That is a little confusing and I vote we never do it again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bf407-7785-4590-99d6-6b0efabd5451",
   "metadata": {},
   "source": [
    "# Define the environment as a class\n",
    "\n",
    "Implement a custom environment class that encapsulates the dynamics of your network simulation. This class should provide methods to interact with the environment, such as reset() to reset the environment to its initial state and step(action) to take a step based on the given action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27344b0-5691-428a-b69e-df270cd11c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import edge_sim_py as esp\n",
    "        \n",
    "def place_and_migrate_to_least_utilized(parameters):\n",
    "    for service in esp.Service.all():\n",
    "        if not service.being_provisioned:\n",
    "            edge_servers = sorted(\n",
    "                esp.EdgeServer.all(), key=lambda s: (\n",
    "                    (s.cpu - s.cpu_demand) \n",
    "                   * (s.memory - s.memory_demand) \n",
    "                   * (s.disk - s.disk_demand)\n",
    "                  ) ** (1/3),\n",
    "                reverse=True,\n",
    "            )\n",
    "            for edge_server in edge_servers: # use ordered list\n",
    "                if edge_server.has_capacity_to_host(service=service):\n",
    "                    if service.server != edge_server:\n",
    "                        # print(f\"[STEP {parameters['current_step']}] Migrating {service} From {service.server} to {edge_server}\")\n",
    "                        service.provision(target_server=edge_server)\n",
    "                        break\n",
    "\n",
    "def stop_on_n_steps(model: object):    \n",
    "    return model.schedule.steps == 30\n",
    "\n",
    "simulator = esp.Simulator(tick_duration=1,tick_unit=\"seconds\",\n",
    "    stopping_criterion=stop_on_n_steps,\n",
    "    resource_management_algorithm=place_and_migrate_to_least_utilized,\n",
    "    scheduler=esp.activation_schedulers.ChaosScheduler,\n",
    ")\n",
    "simulator.initialize(input_file=\"../edgesimpy-tutorials/datasets/sample_dataset2.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca445f8-1f6f-4d17-9c27-3d47bc5c500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 19:10:50,857\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Second argument must be convertable to Trainable', <class '__main__.MyRLAgent'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     31\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Define any necessary configurations for your RL agent\u001b[39;00m\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Register your custom RL agent class with Ray RLlib\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m tune\u001b[38;5;241m.\u001b[39mregister_trainable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m, MyRLAgent)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Train your RL agent using Ray RLlib\u001b[39;00m\n\u001b[1;32m     39\u001b[0m analysis \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     42\u001b[0m     stop\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m},\n\u001b[1;32m     43\u001b[0m     checkpoint_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/edge_chaos/lib/python3.11/site-packages/ray/tune/registry.py:111\u001b[0m, in \u001b[0;36mregister_trainable\u001b[0;34m(name, trainable, warn)\u001b[0m\n\u001b[1;32m    108\u001b[0m     trainable \u001b[38;5;241m=\u001b[39m wrap_function(trainable, warn\u001b[38;5;241m=\u001b[39mwarn)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(trainable, Trainable):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecond argument must be convertable to Trainable\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable)\n\u001b[1;32m    112\u001b[0m _global_registry\u001b[38;5;241m.\u001b[39mregister(TRAINABLE_CLASS, name, trainable)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Second argument must be convertable to Trainable', <class '__main__.MyRLAgent'>)"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "#from ray.rllib.agents import ppo # old \n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "# Define your RL agent class\n",
    "class MyRLAgent:\n",
    "    def __init__(self, config):\n",
    "        # Initialize your RL agent with any necessary parameters\n",
    "        pass\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        # Update your RL agent's policy based on observed experience\n",
    "        pass\n",
    "\n",
    "# Define your resource management algorithm function\n",
    "def resource_management_algorithm(parameters):\n",
    "    # Extract relevant information from the parameters\n",
    "    state = parameters[\"state\"]\n",
    "    \n",
    "    # Use the RL agent to make decisions based on the current state\n",
    "    action = rl_agent.decide(state)\n",
    "    \n",
    "    # Return the action to be taken\n",
    "    return action\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init()\n",
    "\n",
    "# Set up the RL training environment\n",
    "config = {\n",
    "    # Define any necessary configurations for your RL agent\n",
    "}\n",
    "\n",
    "# Register your custom RL agent class with Ray RLlib\n",
    "tune.register_trainable(\"my_agent\", MyRLAgent)\n",
    "\n",
    "# Train your RL agent using Ray RLlib\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config=config,\n",
    "    stop={\"training_iteration\": 100},\n",
    "    checkpoint_at_end=True\n",
    ")\n",
    "\n",
    "# Retrieve the trained RL agent\n",
    "trained_agent = ppo.PPOTrainer(config=config, env=\"your_environment\")\n",
    "\n",
    "# Use the trained agent for inference or further evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7e2d3b-14dc-45f1-b53b-b15c33b7999a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ccf62d9-dcbf-4f6f-b670-53bf608198ce",
   "metadata": {},
   "source": [
    "Ok.... Thinking through a bit \n",
    "\n",
    "\n",
    "Ideally I am a math expert who can help ya with mathy things. I've seen no evidence that people are looking for this skill, but it where I think I should invest. \n",
    "\n",
    "The project I have chosen, edge chaos, involves\n",
    "- edgeSimPy, which I'd say I have now learned to an intermediate level \n",
    "- reinforcement learning, which I have learned at a intro level. \n",
    "- Ray, which I am quite confused about. \n",
    "\n",
    "I think that the right place to pu my focus next is Ray. It seems to have this distributed learning and RL things built in, and it seems like I will practice some python things I have not had a chance to practice before. \n",
    "\n",
    "\n",
    "That said, jumping ship is bad. \n",
    "- edge literature I've been researching... how much do I have left? Is it a finite project? \n",
    "- will I get going with ray and forget about edgesimpy? or can I have edgesimpy in mind as I learn Ray? I'll shoot for the latter of course. \n",
    "- ISL.... Aakash is out there, and willing to talk. That reading group could grow, and it is a great way to dwell on these ML basics that I never really learned. \n",
    "\n",
    "I also have this idea that I could take some time to review statistics,\n",
    "- Koosis, wasserman, etc... I think this would take a solid week of full time work. \n",
    "\n",
    "The idea is tomake this like a BOAT. I would take a training story at this point, I think. \n",
    "\n",
    "I have no notes on ray in my github... I must have lost it in the layoff... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53832817-66c2-40b0-8ee4-d71fba1c06ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b489bb90-616a-44c5-8b30-8850cbb7516e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ba278bb-c366-4cb4-a583-68b2d6633e10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107a69c-6a95-4a59-91f5-8437376e4d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edge_chaos",
   "language": "python",
   "name": "edge_chaos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
